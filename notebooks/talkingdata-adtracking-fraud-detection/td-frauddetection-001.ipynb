{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of project\n",
    "<a href=\"https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection)\" target=\"_blank\">L</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os, sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aisimplekit helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ## Occasionally (dev purpose only)\n",
    "    sys.path.insert(0, \"../..\")\n",
    "    import aisimplekit\n",
    "except ModuleNotFoundError as err:\n",
    "    print(\"\"\"[err] {err}\"\"\".format(err=err))\n",
    "    print(\"\"\"Try: `pip install aisimplekit`\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisimplekit.features.stats import *\n",
    "from aisimplekit.models.lgb import lgb_train_cv\n",
    "from aisimplekit.models.xgb import xgb_train_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom notebook-specific helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(frm, to, debug=True, test_ofst=0, dtypes={}):\n",
    "    \"\"\" \"\"\"\n",
    "    print('Loading train: #%d' % (to-frm))\n",
    "    df_train = pd.read_csv(TRAIN_PATH, parse_dates=['click_time'], skiprows=range(1,frm), nrows=to-frm, dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "    if debug != 0:\n",
    "        nrows = 100000\n",
    "        print('Loading test: #%d' % nrows)\n",
    "        test_df = pd.read_csv(TEST_PATH, skiprows=range(1,1+test_ofst), nrows=nrows, parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])        \n",
    "    elif debug == 0:\n",
    "        print('Loading test: all. Param test_ofst ignored.')\n",
    "        test_df = pd.read_csv(TEST_PATH, parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "    else: raise\n",
    "\n",
    "        # copy reference field\n",
    "    sub = pd.DataFrame()\n",
    "    sub['click_id'] = test_df['click_id'].astype('int')\n",
    "    len_train = len(df_train)\n",
    "    df_train = df_train.append(test_df)\n",
    "    return (df_train, test_df, len_train, sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stats_features(df_train, predictors=['app', 'device', 'os', 'channel', 'hour', 'day']):\n",
    "    \"\"\" \"\"\"\n",
    "    print('Extracting stats features...')\n",
    "    df_train = do_countuniq( df_train, ['ip'], 'app', 'ip_uniq_app_count', 'uint8', show_max=True )\n",
    "    df_train = do_countuniq( df_train, ['ip'], 'channel', 'ip_uniq_chan_count', 'uint8', show_max=True )\n",
    "    df_train = do_count( df_train, ['ip', 'app'], 'ip_app_count', show_max=True )\n",
    "    gc.collect()\n",
    "\n",
    "    predictors.extend([col for col in df_train.columns if col.startswith('X')])\n",
    "    predictors.extend([col for col in df_train.columns if col.startswith('Z')])\n",
    "\n",
    "    if 'ip_tcount' in df_train.columns:\n",
    "        predictors.extend(['ip_tcount'])\n",
    "    if 'ip_app_count' in df_train.columns:\n",
    "        predictors.extend(['ip_app_count'])\n",
    "    if 'ip_app_os_count' in df_train.columns:\n",
    "        predictors.extend(['ip_app_os_count'])\n",
    "    if 'ip_tchan_count' in df_train.columns:\n",
    "        predictors.extend(['ip_tchan_count'])\n",
    "    if 'ip_app_os_var' in df_train.columns:\n",
    "        predictors.extend(['ip_app_os_var'])\n",
    "    if 'ip_app_channel_var_day' in df_train.columns:\n",
    "        predictors.extend(['ip_app_channel_var_day'])\n",
    "    if 'ip_app_channel_mean_hour' in df_train.columns:\n",
    "        predictors.extend(['ip_app_channel_mean_hour'])\n",
    "\n",
    "    return df_train, predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timeserie_features(df_train, predictors=['app', 'device', 'os', 'channel', 'hour', 'day']):\n",
    "    \"\"\" \"\"\"\n",
    "    ## Timeseries features\n",
    "    print('Extracting Timserie features...')\n",
    "\n",
    "    print('[1/2] Extracting nextClick')\n",
    "    D=2**26\n",
    "    df_train['category'] = (df_train['ip'].astype(str) + \"_\" + df_train['app'].astype(str) + \"_\" + df_train['device'].astype(str) \\\n",
    "        + \"_\" + df_train['os'].astype(str)).apply(hash) % D\n",
    "    click_buffer= np.full(D, 3000000000, dtype=np.uint32)\n",
    "\n",
    "    df_train['epochtime'] = df_train['click_time'].astype(np.int64) // 10 ** 9\n",
    "    next_clicks= []\n",
    "    for category, t in zip(reversed(df_train['category'].values), reversed(df_train['epochtime'].values)):\n",
    "        next_clicks.append(click_buffer[category]-t)\n",
    "        click_buffer[category]= t\n",
    "    del(click_buffer)\n",
    "    qq = list(reversed(next_clicks))\n",
    "\n",
    "    df_train.drop(['category'], axis=1, inplace=True)\n",
    "    df_train['nextClick'] = pd.Series(qq).astype('float32')\n",
    "    predictors.append('nextClick')\n",
    "\n",
    "    print('[2/2] Extracting nextClick_sameChan')\n",
    "    D=2**26\n",
    "    df_train['category'] = (df_train['ip'].astype(str) + \"_\" + df_train['channel'].astype(str) + \"_\" + df_train['device'].astype(str) \\\n",
    "        + \"_\" + df_train['os'].astype(str)).apply(hash) % D\n",
    "    click_buffer= np.full(D, 3000000000, dtype=np.uint32)\n",
    "\n",
    "    next_clicks= []\n",
    "    for category, t in zip(reversed(df_train['category'].values), reversed(df_train['epochtime'].values)):\n",
    "        next_clicks.append(click_buffer[category]-t)\n",
    "        click_buffer[category]= t\n",
    "    del(click_buffer)\n",
    "    qq = list(reversed(next_clicks))\n",
    "\n",
    "    df_train.drop(['category'], axis=1, inplace=True)\n",
    "    df_train['nextClick_sameChan'] = pd.Series(qq).astype('float32')\n",
    "    predictors.append('nextClick_sameChan')\n",
    "\n",
    "    return df_train, predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data stored in Kaggle\n",
    "#TRAIN_PATH = \".kaggle/competitions/talkingdata-adtracking-fraud-detection/train.csv\"\n",
    "#TEST_PATH = \".kaggle/competitions/talkingdata-adtracking-fraud-detection/test.csv\"\n",
    "\n",
    "## Data downloaded locally\n",
    "TRAIN_PATH = \"../../data/td-frauddetection-001/train.csv\"\n",
    "TEST_PATH = \"../../data/td-frauddetection-001/test.csv\"\n",
    "\n",
    "dtypes = {\n",
    "    'ip': 'uint32', 'app': 'uint16', 'device': 'uint16',\n",
    "    'os': 'uint16', 'channel': 'uint16', 'is_attributed': 'uint8',\n",
    "    'click_id': 'uint32',\n",
    "}\n",
    "\n",
    "debug = 2\n",
    "limit_features = False\n",
    "\n",
    "nrows=184903891-1\n",
    "frm = 144903891\n",
    "nchunk = 1000000 #2000000\n",
    "val_size=int(0.33*nchunk); # debug == 2\n",
    "df_val = None\n",
    "\n",
    "if debug == 0:\n",
    "    ## No cross-validation, all test data\n",
    "    nchunk = 40000000; val_size = 5000000\n",
    "    frm = 21500000 # day 2/4\n",
    "elif debug == 1:\n",
    "    ## With cross-validation\n",
    "    nchunk = 5000000; frm = 85000000 # day-1\n",
    "    val_size = 2000000; frm_val = 144903891\n",
    "\n",
    "## Train data boundaries (fraction corresponding to nchunk size).\n",
    "to = frm + nchunk\n",
    "test_ofst = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train: #1000000\n",
      "Loading test: #100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "############################################################################################################\n",
    "#        LOADING DATA\n",
    "############################################################################################################\n",
    "(df_train, test_df, len_train, sub) = load_datasets(\n",
    "    frm, to, debug=debug,\n",
    "    test_ofst=test_ofst, dtypes=dtypes\n",
    ")\n",
    "\n",
    "if debug == 1:\n",
    "    print('************ Cross-validation: Loading data (#%d samp) ************'.format(val_size))\n",
    "    len_train = len(df_train) - len(test_df)\n",
    "#    dtypes = { 'ip': 'uint32', 'app': 'uint16', 'device': 'uint16', 'os': 'uint16', 'channel': 'uint16', 'is_attributed' : 'uint8', 'click_id': 'uint32'}\n",
    "    df_val = pd.read_csv(TRAIN_PATH, parse_dates=['click_time'], skiprows=range(1,frm_val), nrows=val_size, dtype=dtypes,\n",
    "                         usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "    df_train = df_train.append(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting stats features...\n",
      "Counting unique  app  by  ['ip'] ...\n",
      "ip_uniq_app_count max value =  54\n",
      "Counting unique  channel  by  ['ip'] ...\n",
      "ip_uniq_chan_count max value =  93\n",
      "Aggregating by  ['ip', 'app'] ...\n",
      "ip_app_count max value =  1142\n",
      "CPU times: user 3.02 s, sys: 96.2 ms, total: 3.11 s\n",
      "Wall time: 3.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "############################################################################################################\n",
    "#        FEATURE EXTRACTION (1/2: base stats)\n",
    "############################################################################################################\n",
    "df_train['hour'] = pd.to_datetime(df_train.click_time).dt.hour.astype('uint8')\n",
    "df_train['day'] = pd.to_datetime(df_train.click_time).dt.day.astype('uint8')\n",
    "df_train['minute'] = pd.to_datetime(df_train.click_time).dt.minute.astype('uint8')\n",
    "\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour', 'day']\n",
    "predictors = ['app', 'device', 'os', 'channel', 'hour', 'day']\n",
    "\n",
    "df_train, predictors = extract_stats_features(df_train, predictors=predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feature: next click period]\n",
      "Extracting Timserie features...\n",
      "[1/2] Extracting nextClick\n",
      "[2/2] Extracting nextClick_sameChan\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_train_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_train_2' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "############################################################################################################\n",
    "#        FEATURE EXTRACTION (1/2: timserie stats)\n",
    "############################################################################################################\n",
    "print('[Feature: next click period]')\n",
    "(df_train, predictors) = extract_timeserie_features(df_train, predictors)\n",
    "\n",
    "df_train['minute'] = pd.to_datetime(df_train.click_time).dt.minute.astype('uint8')\n",
    "prev_len = len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "#        FEATURE SELECTION\n",
    "############################################################################################################\n",
    "if limit_features is True:\n",
    "    predictors = ['app','channel', 'X3', 'X0', 'nextClick',\n",
    "                  'os', 'nextClickPeriod', 'device', 'hour','day',\n",
    "                  'nextClick_sameChan', 'ip_app_count']\n",
    "    categorical = ['app','channel','os','device','hour','day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['click_time'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3938\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3780\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3810\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4964\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4965\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4966\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4967\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['click_time'] not found in axis\""
     ]
    }
   ],
   "source": [
    "%%time\n",
    "############################################################################################################\n",
    "#        TRAINING (1/3: Prepare data for LGB)\n",
    "############################################################################################################\n",
    "## Drop unnecessary columns\n",
    "df_train.drop(['click_time'], axis=1, inplace=True)\n",
    "df_train.drop(['epochtime'], axis=1, inplace=True)\n",
    "\n",
    "## Convert types\n",
    "df_train['ip_uniq_app_count'] = df_train['ip_uniq_app_count'].astype('uint16')\n",
    "df_train['ip_app_count'] = df_train['ip_app_count'].astype('uint16')\n",
    "df_train['ip_uniq_chan_count'] = df_train['ip_uniq_chan_count'].astype('uint16')\n",
    "\n",
    "## Learning Parmeters: LGB\n",
    "params = {\n",
    "    'learning_rate': 0.05,\n",
    "    #'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "    'num_leaves': 15,  # 2^max_depth - 1\n",
    "    'max_depth': 4,  # -1 means no limit\n",
    "    'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'max_bin': 100,  # Number of bucbketed bin for feature values\n",
    "    'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    'scale_pos_weight': 50, # because training data is extremely unbalanced \n",
    "}\n",
    "\n",
    "## Target columns\n",
    "target = 'is_attributed'\n",
    "\n",
    "## Categorical columns\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour', 'day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670000, 15) (330000, 15) (100000, 15)\n",
      "Predictors:  ['app', 'device', 'os', 'channel', 'hour', 'day', 'ip_app_count', 'nextClick', 'nextClick_sameChan']\n",
      "Categorical: ['app', 'device', 'os', 'channel', 'hour', 'day']\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "#        TRAINING (2/3: Train/Test/CV Split)\n",
    "############################################################################################################\n",
    "test_df = df_train[len_train:]\n",
    "\n",
    "if debug == 1:\n",
    "    df_val = df_train[-val_size:]\n",
    "    df_train = df_train[:len_train]\n",
    "else:\n",
    "    df_val = df_train[(len_train-val_size):len_train]\n",
    "    df_train = df_train[:(len_train-val_size)]\n",
    "\n",
    "print(df_train.shape, df_val.shape, test_df.shape)\n",
    "print('Predictors:  %s' % predictors)\n",
    "print('Categorical: %s' % categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "#        TRAINING (3/3: Train LGB Model)\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[10]\ttrain's auc: 0.84027\tvalid's auc: 0.819636\n",
      "[20]\ttrain's auc: 0.895856\tvalid's auc: 0.848605\n",
      "[30]\ttrain's auc: 0.887147\tvalid's auc: 0.919368\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's auc: 0.942482\tvalid's auc: 0.923254\n",
      "\n",
      "Model Report\n",
      "bst1.best_iteration:  1\n",
      "auc: 0.9232537071703216\n"
     ]
    }
   ],
   "source": [
    "(bst, best_iteration, eval_score) = lgb_train_cv(\n",
    "    params, df_train, df_val, predictors, target, \n",
    "    objective='binary', \n",
    "    metrics='auc',\n",
    "    early_stopping_rounds=30,\n",
    "    verbose_eval=True, \n",
    "    num_boost_round=1000,\n",
    "    categorical_features=categorical\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/usr/local/lib/python3.7/dist-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.920701\tvalid-auc:0.904953\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 25 rounds.\n",
      "[5]\ttrain-auc:0.944551\tvalid-auc:0.932233\n",
      "[10]\ttrain-auc:0.960622\tvalid-auc:0.951792\n",
      "[15]\ttrain-auc:0.966551\tvalid-auc:0.95693\n",
      "[20]\ttrain-auc:0.968442\tvalid-auc:0.958544\n",
      "[25]\ttrain-auc:0.971583\tvalid-auc:0.959531\n",
      "[30]\ttrain-auc:0.974489\tvalid-auc:0.960434\n",
      "[35]\ttrain-auc:0.977473\tvalid-auc:0.961272\n",
      "[40]\ttrain-auc:0.979203\tvalid-auc:0.961788\n",
      "[45]\ttrain-auc:0.981173\tvalid-auc:0.961983\n",
      "[50]\ttrain-auc:0.983022\tvalid-auc:0.961731\n",
      "[55]\ttrain-auc:0.984093\tvalid-auc:0.9627\n",
      "[60]\ttrain-auc:0.984602\tvalid-auc:0.962681\n",
      "[65]\ttrain-auc:0.985627\tvalid-auc:0.962492\n",
      "[70]\ttrain-auc:0.986707\tvalid-auc:0.96227\n",
      "[75]\ttrain-auc:0.98752\tvalid-auc:0.96242\n",
      "Stopping. Best iteration:\n",
      "[52]\ttrain-auc:0.983734\tvalid-auc:0.962895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {} # use default ones\n",
    "\n",
    "model = xgb_train_cv(\n",
    "    xgb_params, df_train, df_val, predictors, target,\n",
    "    objective='binary:logistic',\n",
    "    early_stopping_rounds=25,\n",
    "    num_boost_round=200,\n",
    "    verbose_eval=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure XGB Performance on cross-validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.962895298413186\n"
     ]
    }
   ],
   "source": [
    "## validation data\n",
    "import xgboost as xgb\n",
    "dvalid = xgb.DMatrix(df_val[predictors], df_val.is_attributed)\n",
    "\n",
    "## predictions on val\n",
    "pred = model.predict(dvalid, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "## Roc score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "score = roc_auc_score(df_val.is_attributed, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving output to: out-dev-small.csv\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "#        PREDICTIONS on TEST data.\n",
    "############################################################################################################\n",
    "pred = bst.predict(test_df[predictors], num_iteration=best_iteration)\n",
    "\n",
    "fnames = {\n",
    "    0: 'final-output.csv',\n",
    "    1: 'out-dev-big.csv',\n",
    "    2: 'out-dev-small.csv'\n",
    "}\n",
    "\n",
    "fname = fnames[debug]\n",
    "print('Saving output to: {fname}'.format(fname=fname))\n",
    "sub['is_attributed'] = pred\n",
    "sub.to_csv(fname,index=False,float_format='%.9f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "td-frauddetection-001.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
