{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avito demand prediction challenge\n",
    "\n",
    "Avito is one of Russia’s largest classified advertisements website.\n",
    "The idea of this challenge is to predict the demand for a product.\n",
    "\n",
    "<h3>Details:</h3>\n",
    "<p>In their fourth Kaggle competition, Avito is challenging you to predict demand for an online advertisement based on its full description (title, description, images, etc.), its context (geographically where it was posted, similar ads already posted) and historical demand for similar ads in similar contexts. With this information, Avito can inform sellers on how to best optimize their listing and provide some indication of how much interest they should realistically expect to receive.\n",
    "</p>\n",
    "<h3>Link:</h3>\n",
    "<p>Full description of the challenge is available here:</p>\n",
    "<a href=\"https://www.kaggle.com/c/avito-demand-prediction/overview\" target=\"_blank\">https://www.kaggle.com/c/avito-demand-prediction/overview</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "# Models Packages\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "# Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "# Tf-Idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.corpus import stopwords\n",
    "# Viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ## Occasionally (dev purpose only)\n",
    "    sys.path.insert(0, \"../..\")\n",
    "    import aisimplekit\n",
    "except ModuleNotFoundError as err:\n",
    "    print(\"\"\"[err] {err}\"\"\".format(err=err))\n",
    "    print(\"\"\"Try: `pip install aisimplekit`\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisimplekit.features.stats import *\n",
    "from aisimplekit.utils.memory import reduce_mem_usage\n",
    "from aisimplekit.cv.cv_kfold import cross_validate\n",
    "import aisimplekit.features.tfidf as tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: Loading data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(gp, downsample_ratio=None, folder='../input/avito-demand-prediction'):\n",
    "    \"\"\" \"\"\"\n",
    "    print('Loading train/test')\n",
    "    train = pd.read_csv(folder+'/train.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "    test = pd.read_csv(folder+'/test.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "    train_index = train.index\n",
    "    test_index = test.index\n",
    "\n",
    "    if downsample_ratio is not None:\n",
    "        print('Downsampling: %s' % downsample_ratio)\n",
    "        assert downsample_ratio > 1.0\n",
    "        from sklearn.utils import resample\n",
    "        train = resample(train, replace=False, n_samples=int(len(train)/downsample_ratio), random_state=123)\n",
    "        test = resample(test, replace=False, n_samples=int(len(test)/downsample_ratio), random_state=123)\n",
    "        train_index = train.index\n",
    "        test_index = test.index\n",
    "\n",
    "    if gp is not None:\n",
    "        print('Merging train/gp and test/gp')\n",
    "        train = train.reset_index().merge(gp, on='user_id', how='left').set_index('item_id')\n",
    "        test = test.reset_index().merge(gp, on='user_id', how='left').set_index('item_id')\n",
    "\n",
    "    return (train, test, train_index, test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers: Feature Extraction: User aggregated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aggregated_metrics(folder='../input/avito-demand-prediction', save_features=False):\n",
    "    \"\"\" Computes 3 features:\n",
    "    - `avg_times_up_user` - how often the average item of the user has been put up for sale.\n",
    "    - `avg_days_up_user` - the average number of days an item from the user has been put up for sale.\n",
    "    - `n_user_items` - the number of items the user has put up for sale.\n",
    "    \"\"\"\n",
    "    print('1/6 Loading data')\n",
    "    used_cols = ['item_id', 'user_id', 'price']\n",
    "    train = pd.read_csv(folder+'/train.csv', usecols=used_cols)\n",
    "    train_active = pd.read_csv(folder+'/train_active.csv', usecols=used_cols)\n",
    "    test = pd.read_csv(folder+'/test.csv', usecols=used_cols)\n",
    "    test_active = pd.read_csv(folder+'/test_active.csv', usecols=used_cols)\n",
    "\n",
    "    print('2.1/6 Building concatenated dataframe: all_samples')\n",
    "    all_samples = pd.concat([train,train_active,test,test_active]).reset_index(drop=True)\n",
    "    all_samples.drop_duplicates(['item_id'], inplace=True)\n",
    "    del train_active; del test_active; gc.collect()\n",
    "\n",
    "    print('2.2/Aggregating price by user')\n",
    "    gp2 = None\n",
    "    gp3 = None\n",
    "    if False:\n",
    "        gp2 = all_samples.groupby('user_id')['price'].mean()\n",
    "        gp3 = all_samples.groupby('user_id')['price'].max()\n",
    "    all_samples.drop(['price'], inplace=True, axis=1)\n",
    "    \n",
    "    ## concatenate the train and test period data to one dataframe for easier processing\n",
    "    print('2.3/6 Loading/Building concatenated dataframe: all_periods')\n",
    "    train_periods = pd.read_csv(folder+'/periods_train.csv', parse_dates=['date_from', 'date_to'])\n",
    "    test_periods = pd.read_csv(folder+'/periods_test.csv', parse_dates=['date_from', 'date_to'])\n",
    "    all_periods = pd.concat([train_periods,test_periods])\n",
    "    del train_periods; del test_periods; gc.collect()\n",
    "\n",
    "    ## Compute features\n",
    "    print('3/6 Computing: days_up, days_up_sum, times_put_up')\n",
    "    all_periods['days_up'] = all_periods['date_to'].dt.dayofyear - all_periods['date_from'].dt.dayofyear\n",
    "\n",
    "    gp = all_periods.groupby(['item_id'])[['days_up']]\n",
    "    gp_df = pd.DataFrame()\n",
    "    gp_df['days_up_sum'] = gp.sum()['days_up']\n",
    "    gp_df['times_put_up'] = gp.count()['days_up']\n",
    "    gp_df.reset_index(inplace=True)\n",
    "    gp_df.rename(index=str, columns={'index': 'item_id'})\n",
    "\n",
    "    print('4/6 Merging')\n",
    "    all_periods.drop_duplicates(['item_id'], inplace=True)\n",
    "    all_periods = all_periods.merge(gp_df, on='item_id', how='left')\n",
    "    del gp; del gp_df; gc.collect()\n",
    "\n",
    "    ## We have an interesting but kind of useless feature now. As seen in the second venn diagram, there is no overlap at all between `train_active` (and with that `train_periods`) and `train` concerning *item* IDs.\n",
    "    ## For the feature to become useful, we somehow have to associate an item ID with a user ID.\n",
    "    all_periods = all_periods.merge(all_samples, on='item_id', how='left')\n",
    "\n",
    "    print('5/6 Computing metric 1 and 2')\n",
    "    ## Group items: Metrics 1/3 and 2/3\n",
    "    gp = all_periods.groupby(['user_id'])[['days_up_sum', 'times_put_up']].mean().reset_index() \\\n",
    "        .rename(index=str, columns={'days_up_sum': 'avg_days_up_user', 'times_put_up': 'avg_times_up_user'})\n",
    "    ## Metric 3/3\n",
    "    ## For our last feature, `n_user_items`, we just group by user ID and count the number of items.\n",
    "    ## We have to be careful to use `all_samples` instead of `all_periods` here because the latter does not contain the `train.csv` and `test.csv` samples.\n",
    "    print('6/6 Computing metric 3')\n",
    "    n_user_items = all_samples.groupby(['user_id'])[['item_id']].count().reset_index() \\\n",
    "        .rename(index=str, columns={'item_id': 'n_user_items'})\n",
    "    gp = gp.merge(n_user_items, on='user_id', how='outer')\n",
    "\n",
    "    ## Save the features\n",
    "    if save_features is True:\n",
    "        gp.to_csv('aggregated_features.csv', index=False)\n",
    "    ## Cleanup\n",
    "    del all_samples; del all_periods; del train; del test; gc.collect()\n",
    "    return (gp, gp2, gp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers: Feature Extraction:\n",
    "### User statistic features\n",
    "### Category statistic features\n",
    "### Statistics about geography counts, image counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_user_features(df):\n",
    "    ## Q2 - How many posts per user, divided by posts from his (region or city) ?\n",
    "    df = do_count(df, ['region', 'city'], 'X3', show_max=True);\n",
    "    df = do_count(df, ['user_id'], 'X4', show_max=True)\n",
    "    df['X5'] = df['X4']/df['X3']; df.drop(['X3','X4'],axis=1,inplace=True) # X5: \n",
    "    \"\"\" Q3 - How many posts having a description (or image or title or ...), divided by user's total posts ? \"\"\"\n",
    "    df = do_count(df, ['user_id'], 'X6', show_max=True)\n",
    "    df = do_countuniq(df, ['user_id'], 'image_top_1', 'X7', show_max=True)\n",
    "    df['X8'] = df['X7']/df['X6']; df.drop(['X7','X6'],axis=1,inplace=True) #\n",
    "\n",
    "    cols = ['description_num_chars', 'description_num_words', 'description_num_unique_words', 'description_words_vs_unique',\n",
    "            'title_num_chars', 'title_num_words', 'title_num_unique_words', 'title_words_vs_unique']\n",
    "    for col in cols:\n",
    "        df = do_mean(df, ['user_id'], col, 'mean_user_%s'%col, show_max=True)\n",
    "        df = do_mean(df, ['category_name'], col, 'mean_category_%s'%col, show_max=True)\n",
    "        df['ratio_mean_user-cat_%s'%col] = df['mean_user_%s'%col]/df['mean_category_%s'%col]\n",
    "        df.drop(['mean_user_%s'%col, 'mean_category_%s'%col],axis=1,inplace=True)\n",
    "    return df   ## Unique counts, Means; Variances, Min/max/Median ; Top-ranked (categorical); First/Last .. ; Previous/Next\n",
    "\n",
    "def add_category_features(df):\n",
    "    df['price_rank'] = df.groupby(['category_name'])['price'].rank(ascending=True) # 0.229789\n",
    "    return df\n",
    "\n",
    "def add_other_features(df):\n",
    "    if True:\n",
    "        df = do_count(df, ['region'], 'T0', show_max=True)\n",
    "        df = do_count(df, ['city'], 'T1', show_max=True)\n",
    "        df['T2'] = df['T1']/df['T0']; df.drop(['T0','T1'],axis=1,inplace=True)\n",
    "    if True:\n",
    "        df = do_count(df, ['image_top_1'], 'T5', show_max=True) # 0.229818\n",
    "        df = do_count(df, ['image_top_1', 'category_name'], 'T8', show_max=True) # 0.229776\n",
    "    return df\n",
    "\n",
    "def add_stat_features(df):\n",
    "    df = add_user_features(df)\n",
    "    df = add_category_features(df)\n",
    "    df = add_other_features(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = None\n",
    "DEV = True\n",
    "VALID = False # helps to find the best num_rounds !\n",
    "n_rounds = 2401 # identified during validation, WARNING: set early_stopping_rounds to 50 !\n",
    "#data_dir = '../input/avito-demand-prediction'\n",
    "data_dir = '~/.kaggle/competitions/avito-demand-prediction/'\n",
    "\n",
    "USE_IMAGE_FEATURES = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_memory = False\n",
    "max_features = 8000 # TF-IDF: Counts\n",
    "\n",
    "## Dev settings\n",
    "early_stopping_rounds = 15 # 50\n",
    "learning_rate = 0.05 # 0.019\n",
    "num_leaves = 128\n",
    "downsample_ratio = 8.0\n",
    "\n",
    "with_user_agg = False\n",
    "with_tfidf = True\n",
    "\n",
    "main_model = 'lgb' # 'lgb' or 'dnn'\n",
    "analyzer = 'word' # 'word' or 'char'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV is False:\n",
    "    ## Same settings for VALID and SUBMIT\n",
    "    early_stopping_rounds = 50\n",
    "    learning_rate = 0.019\n",
    "    num_leaves = 250\n",
    "    downsample_ratio = None\n",
    "\n",
    "    with_user_agg = True\n",
    "\n",
    "    if main_model == 'dnn':\n",
    "        with_tfidf = False\n",
    "        \n",
    "    optimize_memory = True\n",
    "    max_features = None\n",
    "    sub_filename = \"lgb-base-2.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading (or computing): user aggregated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using aggregated metrics.\n",
      "CPU times: user 407 µs, sys: 37 µs, total: 444 µs\n",
      "Wall time: 356 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gp = None\n",
    "gp2 = None\n",
    "gp3 = None\n",
    "agg_cols = []\n",
    "\n",
    "if gp is not None:\n",
    "    print('Reusing gp.')\n",
    "elif with_user_agg is True:\n",
    "    (gp, gp2, gp3) = compute_aggregated_metrics(folder=data_dir, save_features=False)\n",
    "    agg_cols = list(gp.columns)[1:]\n",
    "else:\n",
    "    print('Not using aggregated metrics.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/test\n",
      "Downsampling: 8.0\n",
      "Train shape: 187928 Rows, 16 Columns\n",
      "Test shape: 63554 Rows, 16 Columns\n",
      "Combining Train and Test\n",
      "Final dataframe shape: 251482 Rows, 16 Columns\n",
      "CPU times: user 35.1 s, sys: 5.28 s, total: 40.4 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(training, testing, traindex, testdex) = load_data(gp, downsample_ratio=downsample_ratio, folder=data_dir)\n",
    "\n",
    "if gp is not None and optimize_memory is True:\n",
    "    del(gp)\n",
    "    gc.collect()\n",
    "\n",
    "y = training.deal_probability.copy().clip(0.0, 1.0)\n",
    "training.drop(\"deal_probability\",axis=1, inplace=True)\n",
    "print('Train shape: {} Rows, {} Columns'.format(*training.shape))\n",
    "print('Test shape: {} Rows, {} Columns'.format(*testing.shape))\n",
    "print(\"Combining Train and Test\")\n",
    "\n",
    "df = pd.concat([training,testing],axis=0)\n",
    "del training, testing\n",
    "gc.collect()\n",
    "print('Final dataframe shape: {} Rows, {} Columns'.format(*df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-computed image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME !\n",
    "if USE_IMAGE_FEATURES is True:\n",
    "    df_img = pd.read_csv('../input/trainimgfeatv2/train_imgfeat_v2.csv')\n",
    "    df_img = df_img.rename(columns={'Unnamed: 0.1': 'image'})\n",
    "    df_img = df_img.drop('Unnamed: 0', axis=1)\n",
    "    df_img['image'] = df_img['image'].apply(lambda x: x.rstrip('.jpg'))\n",
    "    df_img['image'] = df_img['image'].apply(str)\n",
    "    df = df.reset_index().merge(df_img, on='image', how='left').set_index('item_id')\n",
    "    for col in df_img.columns:\n",
    "        df[col].fillna(-1.0, inplace=True)\n",
    "    del(df_img)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1/4] Feature Engineering: Simple transformer features (np.log, dates) + encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering\n",
      "Creating Time Variables..\n",
      "Encoding categorical variables\n",
      "CPU times: user 3.54 s, sys: 63.2 ms, total: 3.6 s\n",
      "Wall time: 3.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Feature Engineering\")\n",
    "df[\"price\"] = np.log(df[\"price\"]+0.001)\n",
    "df[\"price\"].fillna(-999,inplace=True)\n",
    "df[\"image_top_1\"].fillna(-999,inplace=True)\n",
    "\n",
    "print(\"Creating Time Variables..\")\n",
    "df[\"Weekday\"] = df['activation_date'].dt.weekday\n",
    "df[\"Weekd of Year\"] = df['activation_date'].dt.week\n",
    "df[\"Day of Month\"] = df['activation_date'].dt.day\n",
    "\n",
    "# Create Validation Index and Remove Dead Variables\n",
    "# training_index = df.loc[df.activation_date<=pd.to_datetime('2017-04-07')].index\n",
    "# validation_index = df.loc[df.activation_date>=pd.to_datetime('2017-04-08')].index\n",
    "df.drop([\"activation_date\", \"image\"],axis=1,inplace=True)\n",
    "\n",
    "print(\"Encoding categorical variables\")\n",
    "categorical = [\"user_id\", \"region\", \"city\", \"parent_category_name\", \"category_name\",\n",
    "               \"user_type\", \"image_top_1\", \"param_1\", \"param_2\", \"param_3\"]\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for col in categorical:\n",
    "    df[col] = lbl.fit_transform(df[col].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2/4] Feature Engineering: Simple Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.7 s, sys: 25.2 ms, total: 15.7 s\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Meta Text Features\n",
    "textfeats = [\"description\", \"title\"]\n",
    "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "\n",
    "for cols in textfeats:\n",
    "    df[cols] = df[cols].astype(str) \n",
    "    df[cols] = df[cols].astype(str).fillna('missing') # FILL NA\n",
    "    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n",
    "    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] / df[cols+'_num_words'] * 100 # Count Unique Words\n",
    "    df[cols + '_num_chars'] = df[cols].apply(len) # Count number of Characters\n",
    "    df[cols + '_num_desc_punct'] = df[cols].apply(lambda x: count(x, set(string.punctuation)))\n",
    "\n",
    "for col in agg_cols:\n",
    "    df[col].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3/4] Feature Engineering: Stats features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating by  ['region', 'city'] ...\n",
      "X3 max value =  10743\n",
      "Aggregating by  ['user_id'] ...\n",
      "X4 max value =  188\n",
      "Aggregating by  ['user_id'] ...\n",
      "X6 max value =  188\n",
      "Counting unique  image_top_1  by  ['user_id'] ...\n",
      "X7 max value =  91\n",
      "Calculating mean of  description_num_chars  by  ['user_id'] ...\n",
      "mean_user_description_num_chars max value =  3144.0\n",
      "Calculating mean of  description_num_chars  by  ['category_name'] ...\n",
      "mean_category_description_num_chars max value =  810.3571428571429\n",
      "Calculating mean of  description_num_words  by  ['user_id'] ...\n",
      "mean_user_description_num_words max value =  597.0\n",
      "Calculating mean of  description_num_words  by  ['category_name'] ...\n",
      "mean_category_description_num_words max value =  113.14880952380952\n",
      "Calculating mean of  description_num_unique_words  by  ['user_id'] ...\n",
      "mean_user_description_num_unique_words max value =  333.0\n",
      "Calculating mean of  description_num_unique_words  by  ['category_name'] ...\n",
      "mean_category_description_num_unique_words max value =  87.75892857142857\n",
      "Calculating mean of  description_words_vs_unique  by  ['user_id'] ...\n",
      "mean_user_description_words_vs_unique max value =  100.0\n",
      "Calculating mean of  description_words_vs_unique  by  ['category_name'] ...\n",
      "mean_category_description_words_vs_unique max value =  97.67952818096964\n",
      "Calculating mean of  title_num_chars  by  ['user_id'] ...\n",
      "mean_user_title_num_chars max value =  50.0\n",
      "Calculating mean of  title_num_chars  by  ['category_name'] ...\n",
      "mean_category_title_num_chars max value =  31.38918918918919\n",
      "Calculating mean of  title_num_words  by  ['user_id'] ...\n",
      "mean_user_title_num_words max value =  13.0\n",
      "Calculating mean of  title_num_words  by  ['category_name'] ...\n",
      "mean_category_title_num_words max value =  6.42159383033419\n",
      "Calculating mean of  title_num_unique_words  by  ['user_id'] ...\n",
      "mean_user_title_num_unique_words max value =  12.0\n",
      "Calculating mean of  title_num_unique_words  by  ['category_name'] ...\n",
      "mean_category_title_num_unique_words max value =  6.41961637334388\n",
      "Calculating mean of  title_words_vs_unique  by  ['user_id'] ...\n",
      "mean_user_title_words_vs_unique max value =  100.0\n",
      "Calculating mean of  title_words_vs_unique  by  ['category_name'] ...\n",
      "mean_category_title_words_vs_unique max value =  100.0\n",
      "Aggregating by  ['region'] ...\n",
      "T0 max value =  23281\n",
      "Aggregating by  ['city'] ...\n",
      "T1 max value =  10743\n",
      "Aggregating by  ['image_top_1'] ...\n",
      "T5 max value =  19321\n",
      "Aggregating by  ['image_top_1', 'category_name'] ...\n",
      "T8 max value =  3449\n",
      "CPU times: user 12.3 s, sys: 3.59 s, total: 15.9 s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Add statistical features\n",
    "df = add_stat_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4/4] Feature Engineering: TF-IDF Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_spec = {\n",
    "    'description': {\n",
    "        'vectorizer': tfidf.TRANSFORMER_TFIDF,\n",
    "        'ngram_range': (1,2),\n",
    "        'max_features': 17000,\n",
    "        'kwargs': {} # overridable\n",
    "    },\n",
    "    'title': {\n",
    "        'vectorizer': tfidf.TRANSFORMER_COUNT,\n",
    "        'ngram_range': (1,2),\n",
    "        'max_features': max_features,\n",
    "        'kwargs': None # no additional named args\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#vocab_size: 25000\n",
      "CPU times: user 1min 46s, sys: 1.53 s, total: 1min 47s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(vectorizer, df_tfidf, tf_vocab) = tfidf.compute_features(df, transformer_spec, analyzer='word', stop=\"russian\")\n",
    "print('#vocab_size: %d' % len(tf_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model: LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"['description' 'title'] not found in axis\"\n",
      "Memory usage of dataframe is 28.95 MB\n",
      "Memory usage after optimization is: 28.95 MB\n",
      "Decreased by 0.0%\n",
      "Concatenating base features + tfidf features..\n",
      "Training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 15 rounds.\n",
      "[10]\ttrain's rmse: 0.241313\tvalid's rmse: 0.241706\n",
      "[20]\ttrain's rmse: 0.230765\tvalid's rmse: 0.233258\n",
      "[30]\ttrain's rmse: 0.224898\tvalid's rmse: 0.229519\n",
      "[40]\ttrain's rmse: 0.220819\tvalid's rmse: 0.22749\n",
      "[50]\ttrain's rmse: 0.217482\tvalid's rmse: 0.226422\n",
      "[60]\ttrain's rmse: 0.214856\tvalid's rmse: 0.225744\n",
      "[70]\ttrain's rmse: 0.21229\tvalid's rmse: 0.22546\n",
      "[80]\ttrain's rmse: 0.210086\tvalid's rmse: 0.225192\n",
      "[90]\ttrain's rmse: 0.208102\tvalid's rmse: 0.225085\n",
      "[100]\ttrain's rmse: 0.205915\tvalid's rmse: 0.22502\n",
      "[110]\ttrain's rmse: 0.204102\tvalid's rmse: 0.224936\n",
      "[120]\ttrain's rmse: 0.20253\tvalid's rmse: 0.224905\n",
      "[130]\ttrain's rmse: 0.201089\tvalid's rmse: 0.224908\n",
      "Early stopping, best iteration is:\n",
      "[122]\ttrain's rmse: 0.202285\tvalid's rmse: 0.224891\n",
      "RMSE: 0.22489107097022176\n",
      "CPU times: user 13min 47s, sys: 29.6 s, total: 14min 16s\n",
      "Wall time: 6min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if main_model == 'lgb':\n",
    "    # Drop Text Cols\n",
    "    try:\n",
    "        df.drop(textfeats, axis=1, inplace=True)\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "\n",
    "    # Reduce Memory (See function up top)\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    # Combine Dense Features with Sparse Text Bag of Words Features\n",
    "    print('Concatenating base features + tfidf features..')\n",
    "    if df_tfidf is not None:\n",
    "        X = hstack([csr_matrix(df.loc[traindex,:].values), df_tfidf[0:traindex.shape[0]]]) # Sparse Matrix\n",
    "        testing = hstack([csr_matrix(df.loc[testdex,:].values), df_tfidf[traindex.shape[0]:]])\n",
    "        predictors = df.columns.tolist() + tf_vocab\n",
    "    else:\n",
    "        X = hstack([csr_matrix(df.loc[traindex,:].values)]) # Sparse Matrix\n",
    "        testing = hstack([csr_matrix(df.loc[testdex,:].values)])\n",
    "        predictors = df.columns.tolist()\n",
    "\n",
    "    del(df)\n",
    "    del(df_tfidf)\n",
    "    gc.collect()\n",
    "\n",
    "    lgbm_params =  {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse', # 'max_depth': 15,\n",
    "        'num_leaves': num_leaves,\n",
    "        'feature_fraction': 0.50,\n",
    "        'bagging_fraction': 0.70, # 'bagging_freq': 5,\n",
    "        'learning_rate': learning_rate,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    # Training and Validation Set\n",
    "    modelstart = time.time()\n",
    "    print('Training..')\n",
    "    if VALID == True:\n",
    "        print('Mode: Dev with Cross-validation')\n",
    "        ## CV: identify best tuning params, and features\n",
    "        (scores, best_num_rounds) = cross_validate(\n",
    "                                        X.tocsr(), y, folds=3, repeats=1, predictors=predictors,\n",
    "                                        categorical=categorical, lgbm_params=lgbm_params,\n",
    "                                        num_boost_rounds=n_rounds,\n",
    "                                        early_stopping_rounds=early_stopping_rounds, verbose_eval=10\n",
    "                                    )\n",
    "        print(scores, best_num_rounds)\n",
    "        print('Average best round: {}'.format(np.mean(best_num_rounds)))\n",
    "\n",
    "    elif DEV == False:\n",
    "        print('Mode: Submit')\n",
    "        lgtrain = lgb.Dataset(X, y, feature_name=predictors, categorical_feature = categorical)\n",
    "        del(X)\n",
    "        gc.collect()\n",
    "\n",
    "        lgb_clf = lgb.train(lgbm_params, lgtrain, num_boost_round=n_rounds, verbose_eval=40)\n",
    "        lgpred = lgb_clf.predict(testing)\n",
    "\n",
    "        lgsub = pd.DataFrame(lgpred,columns=[\"deal_probability\"],index=testdex)\n",
    "        lgsub['deal_probability'].clip(0.0, 1.0, inplace=True) # Between 0 and 1\n",
    "        lgsub.to_csv(sub_filename,index=True,header=True)\n",
    "    \n",
    "    else:\n",
    "        print('Mode: Dev without Cross-validation')\n",
    "        ## DEV\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=23)\n",
    "        lgtrain = lgb.Dataset(X_train, y_train, feature_name=predictors, categorical_feature = categorical)\n",
    "        lgvalid = lgb.Dataset(X_valid, y_valid, feature_name=predictors, categorical_feature = categorical)\n",
    "        del(X)\n",
    "        del(X_train)\n",
    "        gc.collect()\n",
    "\n",
    "        lgb_clf = lgb.train(lgbm_params, lgtrain, num_boost_round=n_rounds, valid_sets=[lgtrain, lgvalid], valid_names=['train','valid'],\n",
    "            early_stopping_rounds=early_stopping_rounds, verbose_eval=10)\n",
    "\n",
    "        print('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, lgb_clf.predict(X_valid))))\n",
    "        del(X_valid)\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
